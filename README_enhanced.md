# ğŸ”¬ InvestigaciÃ³n de AnÃ¡lisis NumÃ©rico: TÃ©cnicas de Descenso de Gradiente

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![LaTeX](https://img.shields.io/badge/LaTeX-Document-green.svg)](https://www.latex-project.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Este proyecto implementa y compara diferentes tÃ©cnicas de descenso de gradiente estocÃ¡stico y sus variantes para entrenar un modelo de regresiÃ³n logÃ­stica en el dataset Iris.

## ğŸ“ Estructura del Proyecto (Mejorada)

```
InvestigaciÃ³n AN/
â”œâ”€â”€ ğŸ“„ main.py                         # Script principal para ejecutar todo
â”œâ”€â”€ ğŸ“„ requirements.txt                # Dependencias de Python
â”œâ”€â”€ ğŸ“„ README.md                       # Este archivo
â”œâ”€â”€ ğŸ“„ LICENSE                         # Licencia del proyecto
â”‚
â”œâ”€â”€ ğŸ“ src/                            # CÃ³digo fuente modular
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ experiment.py               # Clase principal del experimento
â”‚   â”œâ”€â”€ ğŸ“ optimizers/                 # Implementaciones de optimizadores
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ base.py                 # Clase base abstracta
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sgd.py                  # SGD y SGD+Momentum
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ rmsprop.py              # RMSProp optimizer
â”‚   â”‚   â””â”€â”€ ğŸ“„ adam.py                 # Adam optimizer
â”‚   â””â”€â”€ ğŸ“ utils/                      # Utilidades y herramientas
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ data_utils.py           # Procesamiento de datos
â”‚       â””â”€â”€ ğŸ“„ plotting.py             # Funciones de visualizaciÃ³n
â”‚
â”œâ”€â”€ ğŸ“ config/                         # Configuraciones del proyecto
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â””â”€â”€ ğŸ“„ settings.py                 # ParÃ¡metros y configuraciones
â”‚
â”œâ”€â”€ ğŸ“ docs/                           # DocumentaciÃ³n LaTeX
â”‚   â”œâ”€â”€ ğŸ“„ main.tex                    # Documento principal
|   â”œâ”€â”€ ğŸ“„ presentacion.tex            # PresentaciÃ³n del proyecto
â”‚   â””â”€â”€ ğŸ“„ curvas_convergencia.pdf     # GrÃ¡ficos para LaTeX
â”‚
â”œâ”€â”€ ğŸ“ scripts/                        # Scripts de automatizaciÃ³n
â”‚   â”œâ”€â”€ ğŸ“„ make.py                     # Script tipo "Makefile"
â”‚   â”œâ”€â”€ ğŸ“„ compile_document.py         # Compilador LaTeX mejorado
â”‚   â”œâ”€â”€ ğŸ“„ compile_latex.bat           # Script batch original
â”‚   â”œâ”€â”€ ğŸ“„ create_latex_plots.py       # Generador de grÃ¡ficos
â”‚   â””â”€â”€ ğŸ“„ run_experiment.py           # Ejecutor de experimentos
â”‚
â”œâ”€â”€ ğŸ“ results/                        # Resultados y salidas
â”‚   â”œâ”€â”€ ğŸ“„ experiment_results.txt      # Resultados detallados
â”‚   â””â”€â”€ ğŸ“ figures/                    # GrÃ¡ficos generados
â”‚       â”œâ”€â”€ ğŸ“„ convergencia_optimizadores.png
â”‚       â””â”€â”€ ğŸ“„ convergencia_optimizadores.pdf
â”‚
â””â”€â”€ ğŸ“ legacy/                         # Archivos originales (opcional)
    â”œâ”€â”€ ğŸ“„ gradient_descent_experiment.py
    â””â”€â”€ ğŸ“„ create_pdf_plot.py
```

## ğŸš€ Inicio RÃ¡pido

### MÃ©todo 1: Script Principal (Recomendado)
```bash
# Ejecutar todo el pipeline
python main.py
```

### MÃ©todo 2: Usando el Sistema de AutomatizaciÃ³n
```bash
# Ver comandos disponibles
python scripts/make.py

# Ejecutar pipeline completo
python scripts/make.py all

# Solo experimento
python scripts/make.py experiment

# Solo compilaciÃ³n LaTeX
python scripts/make.py latex
```

## ğŸ“‹ Requisitos

### Python
- Python 3.7+
- numpy >= 1.21.0
- matplotlib >= 3.5.0
- scikit-learn >= 1.0.0

### LaTeX
- DistribuciÃ³n LaTeX (TeX Live, MiKTeX, TinyTeX)
- pdflatex

## ğŸ”§ InstalaciÃ³n

1. **Clonar/Descargar el proyecto**
2. **Instalar dependencias:**
   ```bash
   pip install -r requirements.txt
   # O usando el script de automatizaciÃ³n:
   python scripts/make.py install
   ```

## ğŸ“Š Uso Detallado

### 1. Ejecutar Experimentos Completos
```bash
python main.py
```
Esto ejecutarÃ¡:
- âœ… PreparaciÃ³n de datos del dataset Iris
- âœ… Entrenamiento con todos los optimizadores
- âœ… GeneraciÃ³n de grÃ¡ficos (PNG y PDF)
- âœ… Guardado de resultados detallados

### 2. Compilar Documento LaTeX
```bash
# MÃ©todo mejorado (recomendado)
python scripts/compile_document.py

# MÃ©todo tradicional
python scripts/make.py latex
```

### 3. Generar Solo GrÃ¡ficos
```bash
python scripts/create_latex_plots.py
```

### 4. Limpiar Archivos Temporales
```bash
python scripts/make.py clean
```

## ğŸ§ª ConfiguraciÃ³n de Experimentos

Edita `config/settings.py` para personalizar:

```python
# ParÃ¡metros del experimento
EXPERIMENT_CONFIG = {
    'random_seed': 42,
    'test_size': 0.2,
    'epochs': 30,
    # ...
}

# ConfiguraciÃ³n de optimizadores
OPTIMIZER_CONFIGS = {
    'SGD': {'learning_rate': 0.05},
    'Adam': {'learning_rate': 0.05, 'beta1': 0.9},
    # ...
}
```

## ğŸ“ˆ Algoritmos Implementados

1. **SGD BÃ¡sico**: Descenso de gradiente estocÃ¡stico estÃ¡ndar
2. **SGD + Momentum**: SGD con tÃ©rmino de impulso
3. **RMSProp**: MÃ©todo adaptativo con ajuste de tasa de aprendizaje
4. **Adam**: Combina momentum y RMSProp

## ğŸ“Š Resultados

Los resultados se guardan automÃ¡ticamente en:
- `results/experiment_results.txt` - Resumen detallado
- `results/figures/` - GrÃ¡ficos en PNG y PDF
- `docs/curvas_convergencia.pdf` - GrÃ¡fico para LaTeX

## ğŸ› ï¸ Desarrollo

### Estructura Modular
- **`src/optimizers/`**: Cada optimizador en su propio archivo
- **`src/utils/`**: Funciones de utilidad reutilizables
- **`config/`**: Configuraciones centralizadas
- **`scripts/`**: Scripts de automatizaciÃ³n

### Agregar Nuevo Optimizador
1. Crear archivo en `src/optimizers/`
2. Heredar de `BaseOptimizer`
3. Implementar mÃ©todo `update_weights()`
4. Agregar configuraciÃ³n en `config/settings.py`

## ğŸ› SoluciÃ³n de Problemas

### Error: MÃ³dulos no encontrados
```bash
pip install -r requirements.txt
```

### Error: LaTeX no compilado
```bash
# Verificar instalaciÃ³n
python scripts/compile_document.py
```

### Error: Archivos no encontrados
```bash
# Limpiar y regenerar
python scripts/make.py clean
python scripts/make.py all
```

## ğŸ“ Citas y Referencias

```bibtex
@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}
```

## ğŸ¤ Contribuciones

Este proyecto fue desarrollado como parte de una investigaciÃ³n en AnÃ¡lisis NumÃ©rico sobre mÃ©todos de optimizaciÃ³n en machine learning.

**Autores**: Juan PÃ©rez, Ana GÃ³mez

## ğŸ“„ Licencia

MIT License - Ver archivo `LICENSE` para detalles.

---

â­ **Â¿Te gusta este proyecto?** Â¡Dale una estrella y compÃ¡rtelo!

ğŸ”— **Enlaces Ãºtiles:**
- [DocumentaciÃ³n de NumPy](https://numpy.org/doc/)
- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/)
- [LaTeX Wikibook](https://en.wikibooks.org/wiki/LaTeX)
